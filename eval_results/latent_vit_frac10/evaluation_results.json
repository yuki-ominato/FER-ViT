{
  "accuracy": 0.4172471440512678,
  "classification_report": {
    "Angry": {
      "precision": 0.2969518190757129,
      "recall": 0.31524008350730687,
      "f1-score": 0.3058227848101266,
      "support": 958.0
    },
    "Disgust": {
      "precision": 0.1810344827586207,
      "recall": 0.1891891891891892,
      "f1-score": 0.18502202643171806,
      "support": 111.0
    },
    "Fear": {
      "precision": 0.27044025157232704,
      "recall": 0.251953125,
      "f1-score": 0.2608695652173913,
      "support": 1024.0
    },
    "Happy": {
      "precision": 0.5954773869346733,
      "recall": 0.6679819616685456,
      "f1-score": 0.6296493092454836,
      "support": 1774.0
    },
    "Neutral": {
      "precision": 0.40384615384615385,
      "recall": 0.35766423357664234,
      "f1-score": 0.3793548387096774,
      "support": 1233.0
    },
    "Sad": {
      "precision": 0.29204265791632483,
      "recall": 0.2854851643945469,
      "f1-score": 0.28872668288726683,
      "support": 1247.0
    },
    "Surprise": {
      "precision": 0.5468354430379747,
      "recall": 0.51985559566787,
      "f1-score": 0.5330043183220234,
      "support": 831.0
    },
    "accuracy": 0.4172471440512678,
    "macro avg": {
      "precision": 0.3695183135916839,
      "recall": 0.36962419328630014,
      "f1-score": 0.3689213608033839,
      "support": 7178.0
    },
    "weighted avg": {
      "precision": 0.41159405510886526,
      "recall": 0.4172471440512678,
      "f1-score": 0.41353542217625716,
      "support": 7178.0
    }
  },
  "model_config": {
    "latent_dim": 512,
    "seq_len": 18,
    "embed_dim": 512,
    "depth": 6,
    "heads": 8,
    "mlp_dim": 2048,
    "num_classes": 7,
    "dropout": 0.1
  },
  "checkpoint_path": "experiments/latent_vit_d6_h8_lr0.0001_bs64_ep60_frac10/latent_vit_d6_h8_lr0.0001_bs64_ep60_frac10_20251127_153248/checkpoints/best_model.pt",
  "test_dataset_size": 7178
}