{
  "accuracy": 0.5037614934522151,
  "classification_report": {
    "Angry": {
      "precision": 0.42597087378640774,
      "recall": 0.3663883089770355,
      "f1-score": 0.3939393939393939,
      "support": 958.0
    },
    "Disgust": {
      "precision": 0.4298245614035088,
      "recall": 0.44144144144144143,
      "f1-score": 0.43555555555555553,
      "support": 111.0
    },
    "Fear": {
      "precision": 0.3761574074074074,
      "recall": 0.3173828125,
      "f1-score": 0.3442796610169492,
      "support": 1024.0
    },
    "Happy": {
      "precision": 0.6875699888017918,
      "recall": 0.6922209695603156,
      "f1-score": 0.6898876404494382,
      "support": 1774.0
    },
    "Neutral": {
      "precision": 0.4317862165963432,
      "recall": 0.49797242497972427,
      "f1-score": 0.46252354048964217,
      "support": 1233.0
    },
    "Sad": {
      "precision": 0.37767584097859325,
      "recall": 0.39615076182838815,
      "f1-score": 0.386692759295499,
      "support": 1247.0
    },
    "Surprise": {
      "precision": 0.6453488372093024,
      "recall": 0.6678700361010831,
      "f1-score": 0.6564163217031342,
      "support": 831.0
    },
    "accuracy": 0.5037614934522151,
    "macro avg": {
      "precision": 0.4820476751690506,
      "recall": 0.48277525076971267,
      "f1-score": 0.48132783892137315,
      "support": 7178.0
    },
    "weighted avg": {
      "precision": 0.5015831751472916,
      "recall": 0.5037614934522151,
      "f1-score": 0.5015495977570056,
      "support": 7178.0
    }
  },
  "model_config": {
    "latent_dim": 512,
    "seq_len": 18,
    "embed_dim": 512,
    "depth": 6,
    "heads": 8,
    "mlp_dim": 2048,
    "num_classes": 7,
    "dropout": 0.1
  },
  "checkpoint_path": "experiments/latent_vit_d6_h8_lr0.0001_bs64_ep60_frac50/latent_vit_d6_h8_lr0.0001_bs64_ep60_frac50_20251127_153856/checkpoints/best_model.pt",
  "test_dataset_size": 7178
}