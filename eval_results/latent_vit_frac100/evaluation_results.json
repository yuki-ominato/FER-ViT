{
  "accuracy": 0.5447199777096684,
  "classification_report": {
    "Angry": {
      "precision": 0.4355362946912243,
      "recall": 0.4196242171189979,
      "f1-score": 0.4274322169059011,
      "support": 958.0
    },
    "Disgust": {
      "precision": 0.7105263157894737,
      "recall": 0.4864864864864865,
      "f1-score": 0.5775401069518716,
      "support": 111.0
    },
    "Fear": {
      "precision": 0.44585253456221197,
      "recall": 0.3779296875,
      "f1-score": 0.4090909090909091,
      "support": 1024.0
    },
    "Happy": {
      "precision": 0.6985058697972252,
      "recall": 0.737880496054115,
      "f1-score": 0.7176535087719298,
      "support": 1774.0
    },
    "Neutral": {
      "precision": 0.5220588235294118,
      "recall": 0.4606650446066504,
      "f1-score": 0.48944420508401554,
      "support": 1233.0
    },
    "Sad": {
      "precision": 0.3958197256694971,
      "recall": 0.4859663191659984,
      "f1-score": 0.43628509719222464,
      "support": 1247.0
    },
    "Surprise": {
      "precision": 0.7139364303178484,
      "recall": 0.7027677496991577,
      "f1-score": 0.708308065494239,
      "support": 831.0
    },
    "accuracy": 0.5447199777096684,
    "macro avg": {
      "precision": 0.5603194277652703,
      "recall": 0.5244742858044866,
      "f1-score": 0.5379648727844416,
      "support": 7178.0
    },
    "weighted avg": {
      "precision": 0.5464449011834169,
      "recall": 0.5447199777096684,
      "f1-score": 0.5435705843732256,
      "support": 7178.0
    }
  },
  "model_config": {
    "latent_dim": 512,
    "seq_len": 18,
    "embed_dim": 512,
    "depth": 6,
    "heads": 8,
    "mlp_dim": 2048,
    "num_classes": 7,
    "dropout": 0.1
  },
  "checkpoint_path": "experiments/latent_vit_d6_h8_lr0.0001_bs64_ep60_frac100/latent_vit_d6_h8_lr0.0001_bs64_ep60_frac100_20251127_155430/checkpoints/best_model.pt",
  "test_dataset_size": 7178
}