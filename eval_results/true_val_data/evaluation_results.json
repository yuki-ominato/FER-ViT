{
  "accuracy": 0.5412371134020618,
  "classification_report": {
    "Angry": {
      "precision": 0.4472252448313384,
      "recall": 0.4290187891440501,
      "f1-score": 0.4379328716036228,
      "support": 958.0
    },
    "Disgust": {
      "precision": 0.6590909090909091,
      "recall": 0.5225225225225225,
      "f1-score": 0.5829145728643216,
      "support": 111.0
    },
    "Fear": {
      "precision": 0.41393034825870645,
      "recall": 0.40625,
      "f1-score": 0.41005421389847213,
      "support": 1024.0
    },
    "Happy": {
      "precision": 0.6773373983739838,
      "recall": 0.7514092446448704,
      "f1-score": 0.7124532335649385,
      "support": 1774.0
    },
    "Neutral": {
      "precision": 0.4693572496263079,
      "recall": 0.5093268450932684,
      "f1-score": 0.48852586542201476,
      "support": 1233.0
    },
    "Sad": {
      "precision": 0.42967409948542024,
      "recall": 0.40176423416198875,
      "f1-score": 0.4152507252382926,
      "support": 1247.0
    },
    "Surprise": {
      "precision": 0.7752161383285303,
      "recall": 0.6474127557160048,
      "f1-score": 0.7055737704918033,
      "support": 831.0
    },
    "accuracy": 0.5412371134020618,
    "macro avg": {
      "precision": 0.5531187697135994,
      "recall": 0.5239577701832436,
      "f1-score": 0.536100750440495,
      "support": 7178.0
    },
    "weighted avg": {
      "precision": 0.5413468652254725,
      "recall": 0.5412371134020618,
      "f1-score": 0.5397788533960198,
      "support": 7178.0
    }
  },
  "model_config": {
    "latent_dim": 512,
    "seq_len": 18,
    "embed_dim": 512,
    "depth": 6,
    "heads": 8,
    "mlp_dim": 2048,
    "num_classes": 7,
    "dropout": 0.1
  },
  "checkpoint_path": "experiments/latent_vit_d6_h8_lr0.0001_bs64_ep60/latent_vit_d6_h8_lr0.0001_bs64_ep60_20251106_133118/checkpoints/best_model.pt",
  "test_dataset_size": 7178
}